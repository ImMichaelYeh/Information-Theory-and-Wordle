{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-145c61a3-4078-4ad9-8bcf-ec3fe4a9f18d.json","kernel":"julia-1.7","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":32,"id":"f73dd9","input":"# Recursive example of this binary search\nfunction numberGuesser(rangeStart, rangeEnd, target)\n    currentGuess = div((rangeStart + rangeEnd), 2) # Sets currentGuess to middle of the range\n    println(currentGuess)\n    if currentGuess == target\n        return\n    elseif currentGuess > target\n        numberGuesser(rangeStart, currentGuess - 1, target)\n    else     \n        numberGuesser(currentGuess + 1, rangeEnd, target)\n    end\nend\n\nfor i in 1:5\n    println(string(2*i^2, \": \"))\n    numberGuesser(1, 100, i)\n    println()\nend","output":{"0":{"name":"stdout","output_type":"stream","text":"2: \n50\n25\n12\n6\n3\n1\n\n8: \n50\n25\n12\n6\n3\n1\n2\n\n18: \n50\n25\n12\n6\n3\n\n32: \n50\n25\n12\n6\n3\n4\n\n50: \n50\n25\n12\n6\n3\n4\n5\n\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"93ba01","input":"# This Julia file includes all the words that Wordle will accept as a guess, as well as all the possible answers.\ninclude(\"words.jl\") # I stole this information straight from the Wordle source code. Interestingly, the numbers I have here don't match with the numbers from the 3Blue1Brown video, so I am wondering if there was a change made.\n\nlength(words)","output":{"0":{"data":{"text/plain":"10657"},"exec_count":8,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"e66048","input":"-log(670/10657)/log(2) # Number of bits of information from our example","output":{"0":{"data":{"text/plain":"3.991496463421989"},"exec_count":9,"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"markdown","id":"090893","input":"## Goal of this lecture (Part 1 of 2):","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"26b133","input":"## Basics of Information Theory","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"298b4e","input":"## Entropy!","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"35a174","input":"In Information Theory, the unit of information used is called **the bit**. The amount of bits is usually represented with the letter I (for information) and can be solved with this equation $I = -\\log_2(p)$ where p how much smaller the space of possibilities becomes. Here is an example:","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"3d4c83","input":"This lecture will be split up into 2 parts. This first part will introduce the game wordle and the basic concepts of Information Theory that we will use. All the code written in this part will be in Julia. The 2nd part of this lecture will take our knowledge of Information Theory and combine it with the game Wordle to create an algorithm that would give us an optimal guess of words given the current information.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"41d979","input":"## What is Wordle?","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"4670d5","input":"The goal of the game Wordle is to find the correct word within 6 guesses. However, for most players, we enjoy finding the word in the least amount of guesses possible! So, we want to be able to create an algorithm that would help us achieve this!\n\n\nOne thing that we could try to do is to remove as many possibilities per guess as possible! In a simple guessing game, like trying to guess a number between 1 and 100 and being told if we are high or low! We can see that the least optimal way of guessing would be in order. For example, if the number is 75, then it would take 75 guesses to get the right answer! The optimal way would be to cut the numbers in half each time! Watch how this works:\n1. 50 (low, so well go into the middle of the higher interval)\n2. 75 (correct!)\nWe were able to reduce our guess from 75 to just 2 using this algorithm! In fact, using this algorithm, the highest amount of guesses required would be $\\log_2(n)$ where n is the number of possible guesses. So that would mean that most amount of guesses for our number game is just 7!\n\nThis is actually the equivalent of a binary search on a list.","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"4a028a","input":"## What is our goal?","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"831914","input":"If you don't already know what the game Wordle is, you might be living under a rock. However, if that is true, you don't have to worry. That is because Wordle is a simple word game that operates off of a couple of rules.\n\nIn this game, you'll have 6 guesses to find the hidden 5 letter word. For every letter of each guess, you will be told either:\n\n1. The letter does not exist in the word, marked with a gray box\n2. The letter exists in the word but is not in the right spot, marked with a yellow box\n3. The letter exists in the word and is in the right spot, marked with a green box\n\n**Here is an example:**\nOur first guess was \"CRATE\". Since the letters \"CRAT\" are in a gray box, we know that none of those letters exist in this word. Our next guess is \"DRIED\". Since the first D is now in a green box, we know the word begins with a D! Since there is another D in a yellow box, we know that there is at least another D somewhere in the word! Our final guess is the word \"DODGE\", which shows up in all green which means that we got the word in just 3 guesses!\\\n![Alt text](wordleExample.png)","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"8acca1","input":"## Conclusion for part 1!","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"ce9d5b","input":"# Information Theory and the Game Wordle (Part 1 of 2)","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"d0178f","input":"There is a way for us to calculate the expected information that a word could give us! This function looks like this $E(w) = \\displaystyle\\sum_{x}-p(x)\\log_2(p(x))$. This is called the **Entropy Function**.\\\nHere, $w$ is the word that we are guessing. $x$ is one of the possible results that could appear on the screen, and $p(x)$ is the probability of each of those results.\n\nFor example, if the word is \"CRATE\", the first x could be the result where all letters are gray. Then, $p(x$) is the probability of getting that result, and $-log_2(x)$ is the amount of information of that result. When we take the sum, we get the expected information of each guess.\n\nSo now, we want to find the word with the highest entropy, or the highest expected information!","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"d42ae4","input":"Now that you understand how the game Wordle works, and the basics of Information Theory, we can now write a program that can find us the optimal guess!","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"dfeae0","input":"This is great, but we can't exactly do this with Wordle. With our brains, we could usually guess things that would give us more information, but it is hard to think of what guesses would cut the amount of possible guesses down the most would be!\n\n\nHere is where Information Theory will come in handy!","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"f8515b","input":"There are 10,657 possible guesses that Wordle will accept. In reality, there are less words that it would use as answers, but let's ignore that. Now, let's say that the word CRATE cuts the number of guesses down from 10657 to just 670, then the amount of guesses have been cut down by a factor of almost 16! That means that amount of information that the word CRATE gives us is about $I = -\\log_2(1/16) = 4$! By finding the word for each guess that gives us the most amount of information or bits, we can reduce the number of possible guesses by as much as possible per guess!\n\nSo how can we find which word would give us the most amount of information?","pos":13,"type":"cell"}
{"id":0,"time":1645247465753,"type":"user"}
{"last_load":1645247466446,"type":"file"}